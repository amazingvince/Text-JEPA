# Default configuration for Text-JEPA with C4 dataset

# Model configuration
model:
  name_or_path: "roberta-base"
  hidden_size: 768
  context_encoder_layers: 12
  target_encoder_layers: 12
  predictor_layers: 6
  num_heads: 12
  dropout_prob: 0.1
  use_custom_model: false

# Data configuration
data:
  max_length: 512
  num_spans: 2
  min_span_length: 5
  max_span_length: 20
  min_text_length: 200
  context_mask_ratio: 0.5
  num_workers: 4
  buffer_size: 10000

# Training configuration
training:
  batch_size: 16
  max_steps: 100000
  eval_steps: 1000
  eval_samples: 1000
  save_steps: 5000
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 10000
  max_grad_norm: 1.0
  ema_decay: 0.996  # EMA decay for target encoder update
  scheduler: "linear_warmup_cosine_decay"  # Options: "linear_warmup_cosine_decay", "linear_warmup"